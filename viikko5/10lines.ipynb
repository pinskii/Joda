{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 riviä koodia: Luonnollisen kielen käsittely\n",
    "\n",
    "Tutkitaan demossa 4 annettua [twiittidataa](https://github.com/InfoTUNI/joda2024/blob/main/demo/4.%20tweet_classifier/shared_data/sample_texts.xlsx) ministerien twiiteistä. Tavoitteena on hakea kaikki sanat vektorimuotoon, jolloin voimme tutkia esim. sanatiheyttä."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Lue twiittidata Excel-tiedostosta\n",
    "twiittidata = pd.read_excel('sample_texts.xlsx')\n",
    "\n",
    "# Esikäsittele twiittidata, esimerkiksi poistamalla välimerkit ja muuntamalla pieniksi kirjaimiksi\n",
    "twiittidata['text_nousernames'] = twiittidata['text_nousernames'].apply(lambda x: x.lower())\n",
    "\n",
    "# Luodaan CountVectorizer-olio, joka laskee sanojen esiintymistiheydet\n",
    "# Huomioi yksittäiset sanat, ei huomioida sanaa 'rt', sillä se ei ole osa twiittiä. \n",
    "# Samoin lisäsanoja (esim. artikkelit) ei huomioida, sillä ne eivät tuo kontekstista sisältöä \n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b', stop_words=['rt','the', 'a', 'of', 'ja', 't', 'and', 'https', 'että', 'ei', 'to', 'in', 'on', 'ovat', 'ole', 'co', '2', '5', '7', 'n', 'se', '4', '1', 'for', 'kun', 'niin', 'tai', '3', '6', 'sen', 'oli', 'voi'])  \n",
    "\n",
    "# Sovitetaan ja muunnetaan teksti vektorimuotoon\n",
    "X = vectorizer.fit_transform(twiittidata['text_nousernames'])\n",
    "\n",
    "# Tulostetaan vektorin koko\n",
    "print(\"Vektorin koko:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Halutaan tietää twiiteissä eniten käytetyt sanat. Lajitellaan sanat esiintymistiheyden mukaan ja tulostetaan ne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laitetaan sanat esiintymistiheyden mukaan järjestykseen\n",
    "term_frequencies = zip(vectorizer.get_feature_names_out(), X.sum(axis=0).tolist()[0])\n",
    "sorted_terms = sorted(term_frequencies, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Tulostetaan 10 eniten esiintyvää sanaa\n",
    "print(\"10 eniten esiintyvää sanaa:\")\n",
    "for term in sorted_terms[:10]:\n",
    "    print(term[0], \":\", term[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huomataan, että suurin osa sanoista on yhä edelleen turhia lisäsanoja, vaikka niitä lisäisi montakin aiempaan 'stop_words' -listaan. Opimme siis, että tekstin- ja sanankäsittelyyn tarvitaan paljon huomioita ja esikäsittelyä, jotta siitä saadaan järkevää tietoa irti. \n",
    "\n",
    "Tällä 'stop_words' -listalla saatuun top 10:een kuuluu esim. sana 'tänään', jonka perusteella voidaan arvioida, että twiitit liittyvät usein nykyhetkeen. Myös Suomeen liittyviä sanoja on monessa muodossa, eli (ei) yllätyksellisesti suomalaiset ministerit twiittaavat paljon Suomesta."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
