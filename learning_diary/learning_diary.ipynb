{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oppimispäiväkirja\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 1 - ohjelmien asennus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datatiedeprojektin keskeiset Python-kirjastot:\n",
    "- **Pandas**: Datan manipulaatio ja analyysi\n",
    "- **Scikit-learn**: Luokittelu- ja regressioalgoritmit\n",
    "- **Seaborn**: Datan visuaalinen analyysi\n",
    "- **Holoviews**: Datan visualisointi\n",
    "- **Streamlit**: Koneoppimisen ja datatieteen sovelluskehitys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miten kirjastot kannattaa asentaa?\n",
    "\n",
    "Kirjastojen asennukseen selkein tapa Windowsilla on käyttää command linea eli komentoriviä. Pip on Pythonin asennuksen mukana tuleva pakettien- ja kirjastojenhallintajärjestelmä, jonka avulla ohjelmat on helppo asentaa. Yleisesti kirjastojen asennus toimii komentorivikomennolla \"pip install -kirjaston nimi-\". Näin asensin Pandas-, Seaborn- ja Holoviews-kirjastot. Testataan, että asennukset onnistuivat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn\n",
    "import holoviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huomasin kuitenkin, että Scikit-learn ja Streamlit -kirjastot eivät tue 32-bittistä Windowsia ja Pythonia. En siis itse saanut niitä asennettua, mutta muilla käyttöjärjestelmillä asennus toimisi samalla tavalla kuin yllä. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erilaisia ympäristövaihtoehtoja ovat:\n",
    "- CSC Notebooks\n",
    "- Google Colab\n",
    "- Anaconda\n",
    "- Microsoft Visual Studio Code\n",
    "- Jupyter Lab\n",
    "- pilvipalvelupohjainen analytiikkaympäristö, esim. Google Cloud Platform tai Microsoft Azure\n",
    "\n",
    "**Mitkä ovat niiden vahvuudet ja heikkoudet?**\n",
    "\n",
    "Osa ympäristöistä tukevat pilvipalvelupohjaista virtuaalikoneella tapahtuvaa laskentaa ja tallennusta. Näitä ovat Google Colab ja pilvipalvelupohjaiset ympäristöt. Kun datan käsittely ja laskeminen tehdään muualla, säästyy omalta tietokoneelta resursseja ja myös reaaliaikainen pilveen tallennus on hyötynä. Jupyter Lab on Jupyter Notebookille tarkoitettu ohjelma, joka antaa käyttöön monia ominaisuuksia, kuten erilaisia visualisointeja ja laajan määrän kirjastoja. Jupyter Lab on saman organisaation kehittämä kuin Jupyter Notebook ja siksi se on varma vaihtoehto. Samoin Anaconda on suosittu järjestelmä, sillä se sisältää valmiiksi monia tarpeellisia kirjastoja, esimerkiksi pakettienhallintaan käytetyn conda-järjestelmän. CSC Notebooks on suomalaisen organisaation vaihtoehto, joka myös käyttää ulkoista laskentatietokonetta. Virtual Studio Code sisältää IDEn ja mahdollisuuden ladata monia kirjastoja helposti.\n",
    "\n",
    "Oikeastaan kaikki vaihtoehdot ovat hyviä ja on vaikea sanoa, mikä niistä olisi paras tai huonoin. Eniten eroja on siinä, käytetäänkö ulkoista laskentatehoa ja voiko sisältöä tallentaa helposti pilveen. Myös visuaalisissa puolissa on eroja.\n",
    "\n",
    "**Miten teet valinnan ympäristöjen välillä?**\n",
    "\n",
    "Valitsin itse käyttöön Visual Studio Coden, sillä olen käyttänyt sitä jo paljon aiemmin, eli käyttöönottoon ja opetteluun ei tarvitse käyttää aikaa turhaan. Pidän myös siitä, että mukana tulee kätevä ja selkeäkäyttöinen IDE ja VS Code tukee myös Gitin käyttöä samalla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miten dataa luetaan ja hallinnoidaan?\n",
    "\n",
    "Dataa voidaan lukea esimerkiksi taulukoista tai muista tiedostoista ja tietokannoista. Näitä tiedostoja voivat olla esim. CSV ja JSON -päätteiset tiedostot. Ennen kuin dataa voidaan alkaa analysoimaan, se täytyy esikäsitellä, eli siitä poistetaan epäkelvot ja kaksinkertaiset arvot, se normalisoidaan ja voidaan muotoilla. Sitten dataa voidaan analysoida eri keinoin, esimerkiksi tilastollisella analyysilla. Tätä analyysia voidaan myös visualisoida esimerkiksi yllä mainituilla kirjastoilla. Data-analyysissa tavoitteena voi olla mm. ennustaminen, luokittelu tai ongelmien ratkaiseminen. Dataa hallinoidaan ja säilytetään usein tietokannoissa tai taulukkomuodossa.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 2 - Aineiston kerääminen ja esikäsittely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tämän projektin datalähde\n",
    "\n",
    "Löysin mielenkiintoisen datalähteen Netflixin suosituimmista ohjelmista päivittäin. Dataa on saatavilla tammikuusta 2020 marraskuuhun 2022. Harmillisesti datalähdettä ei voi lukea linkistä suoraan nettisivulta, sillä sen lataaminen vaati sisäänkirjautumista. Datalähde on kuitenkin ladattuna samassa kansiossa kuin tämä päiväkirja nimellä 'netflix_daily_top_10.csv'. Datan voi myös löytää [tältä sivulta](https://www.kaggle.com/datasets/prasertk/netflix-daily-top-10-in-us?resource=download). \n",
    "\n",
    "Toinen datalähde, jota mahdollisesti käytän projektissa, on IMDB:n arvostelut ohjelmista. Datalähde löytyy tästä kansiosta nimellä 'title.ratings.tsv.gz'. Jotta arvostelujen lähde saadaan toimimaan, pitää lukea myös IMDB:n toista datalähdettä, josta saamme ohjelmien otsikot tekstimuodossa, sillä 'title.ratings.tsv.gz' -datalähteessä yksittäinen ohjelma on pelkästään identifioitu numerosarjalla. Tämä toinen IMDB:n datalähde on nimeltään 'title.basics.tsv.gz'. Molemmat IMDB:n datalähteet voi myös löytää [tältä sivulta](https://developer.imdb.com/non-commercial-datasets/).\n",
    "\n",
    "Ajatuksena olisi yhdistää näitä Netlixin ja IMDB:n datalähdeitä jotenkin. Tällä hetkellä ideana on verrata Netflixin ohjelmien päivittäistä suosiota IMDB:n arvostelunumeroihin ja löytää, onko näillä jokin suhde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datan lukeminen Pythonilla\n",
    "\n",
    "Pythonilla yleisin datan lukemiseen käytetty kirjasto on Pandas. Pandasilla voi lukea tietoa suoraan erilaisista datatiedostoista, esim. CSV ja JSON tai luoda oman dataluokan kaksiulotteisena taulukkona. Esimerkkinä luku Netflix-ohjelmien .csv-tiedostosta: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(\"netflix_daily_top_10.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luetaan data myös IMDB:n datalähteestä:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Define file paths\\nbasics_file = 'title.basics.tsv.gz'\\nratings_file = 'title.ratings.tsv.gz'\\nnetflix_file = 'netflix_daily_top_10.csv'\\n\\n# Define a smaller chunk size for basics file\\nbasics_chunksize = 10 ** 1  # Adjust this number based on your available memory\\n\\n# Initialize an empty list to store the filtered basics chunks\\nfiltered_basics_chunks = []\\n\\n# Read Netflix file in chunks\\nfor netflix_chunk in pd.read_csv(netflix_file, usecols=['Title'], chunksize=basics_chunksize):\\n    # Get unique titles from the current Netflix chunk\\n    netflix_titles_chunk = netflix_chunk['Title'].unique()\\n\\n    # Read basics file and filter based on title column\\n    for basics_chunk in pd.read_csv(basics_file, compression='gzip', sep='\\t', usecols=['tconst', 'primaryTitle'], chunksize=basics_chunksize):\\n        basics_chunk_filtered = basics_chunk[basics_chunk['primaryTitle'].isin(netflix_titles_chunk)]\\n        filtered_basics_chunks.append(basics_chunk_filtered)\\n\\n# Concatenate the filtered basics chunks into a single DataFrame\\nfiltered_basics_df = pd.concat(filtered_basics_chunks)\\n\\n# Read ratings file, no need to filter based on title column since we're merging with filtered_basics_df later\\nratings_df = pd.read_csv(ratings_file, compression='gzip', sep='\\t')\\n\\n# Merge filtered_basics_df with ratings_df on the 'tconst' column\\nmerged_df = pd.merge(ratings_df, filtered_basics_df, on='tconst', how='inner')\\n\\n# Display the merged DataFrame\\nprint(merged_df)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Define file paths\n",
    "basics_file = 'title.basics.tsv.gz'\n",
    "ratings_file = 'title.ratings.tsv.gz'\n",
    "netflix_file = 'netflix_daily_top_10.csv'\n",
    "\n",
    "# Define a smaller chunk size for basics file\n",
    "basics_chunksize = 10 ** 1  # Adjust this number based on your available memory\n",
    "\n",
    "# Initialize an empty list to store the filtered basics chunks\n",
    "filtered_basics_chunks = []\n",
    "\n",
    "# Read Netflix file in chunks\n",
    "for netflix_chunk in pd.read_csv(netflix_file, usecols=['Title'], chunksize=basics_chunksize):\n",
    "    # Get unique titles from the current Netflix chunk\n",
    "    netflix_titles_chunk = netflix_chunk['Title'].unique()\n",
    "\n",
    "    # Read basics file and filter based on title column\n",
    "    for basics_chunk in pd.read_csv(basics_file, compression='gzip', sep='\\t', usecols=['tconst', 'primaryTitle'], chunksize=basics_chunksize):\n",
    "        basics_chunk_filtered = basics_chunk[basics_chunk['primaryTitle'].isin(netflix_titles_chunk)]\n",
    "        filtered_basics_chunks.append(basics_chunk_filtered)\n",
    "\n",
    "# Concatenate the filtered basics chunks into a single DataFrame\n",
    "filtered_basics_df = pd.concat(filtered_basics_chunks)\n",
    "\n",
    "# Read ratings file, no need to filter based on title column since we're merging with filtered_basics_df later\n",
    "ratings_df = pd.read_csv(ratings_file, compression='gzip', sep='\\t')\n",
    "\n",
    "# Merge filtered_basics_df with ratings_df on the 'tconst' column\n",
    "merged_df = pd.merge(ratings_df, filtered_basics_df, on='tconst', how='inner')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_df)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muita vaihtoehtoja ovat esimerkiksi Pythonin moduuli \"csv\", jonka reader-funktion avulla voidaan lukea CSV-tiedostoja seuraavanlaisesti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.reader(\"tiedoston nimi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythonissa on myös toinen sisäänrakennettu paketti \"json\", jonka avulla JSON-tiedostoja voi muuttaa Pythonin dictionaryn muotoon. Muutos toimii seuraavasti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(\"tiedoston nimi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Molemmat Pythonin omat moduulit / paketit ovat kuitenkin vaikeampia käyttää kuin Pandas, sillä niissä data pitää käydä rivi kerrallaan läpi, eikä esimerkiksi kokoa taulukon tulostukseen ole suoraa funktiota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datan esittäminen ja DataFrame\n",
    "\n",
    "Koneoppimisprojekteissa data esitetään usein taulukkomuodossa, jossa jokainen rivi vastaa yhtä havaintoa tai esimerkkiä, ja jokainen sarakkeista vastaa havainnon piirrettä tai ominaisuutta. Tämä taulukkomuotoinen esitys voi olla DataFrame, joka on Pandas-kirjaston tarjoama tietorakenne Pythonissa.  Valitussa Netflix-datasetissa jokainen rivi on yksi havainto eli päivä ja jokainen sarake päivän ohjelmien ominaisuus, (esimerkiksi ohjelman nimi).\n",
    "\n",
    "DataFrame on taulukko, joka koostuu riveistä ja sarakkeista. Se tarjoaa kätevän tavan järjestää, analysoida ja manipuloida dataa Pythonissa. DataFrame on erityisen hyödyllinen koneoppimisprojekteissa, koska se mahdollistaa datan lataamisen, esikäsittelyn, visualisoinnin ja mallin sovittamisen helposti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datan hallinta ja to_pickle()\n",
    "\n",
    "To_pickle() -metodi tarjoaa kätevän tavan tallentaa DataFrame-objekti binäärimuodossa, mikä helpottaa tiedon hallintaa. Tallennus on kätevää, kun DataFramen haluaa tallentaa väliaikaisesti tai jakaa sen muille. Pkl-muotoinen tiedosto on myös nopeampi lukea ja tallentaa kuin esimerkiksi csv-tiedosto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 4: Datatiedeprojektin vaiheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datatiedeprojektin vaiheet voivat vaihdella hieman organisaation ja projektin tarpeiden mukaan, mutta yleensä ne seuraavat suunnilleen seuraavaa rakennetta:\n",
    "\n",
    "1. Ongelman määrittely: Ymmärrä liiketoimintaongelma tai tavoite, jota pyrit ratkaisemaan. Selvitä, mitä tietoa tarvitaan ja mitä halutaan saavuttaa.\n",
    "\n",
    "2. Tiedon kerääminen: Hanki tarvittava data. Tämä voi sisältää tietojen keräämistä eri tietolähteistä, kuten tietokannoista, API:sta, tiedostoista jne.\n",
    "\n",
    "3. Dataesikäsittely: Puhdista ja valmistele data analyysia varten. Tähän sisältyy puuttuvien arvojen käsittely, epäkelpojen arvojen poisto, muuttujien skaalaus ja muunnokset, sekä tarvittaessa ominaisuuksien valinta.\n",
    "\n",
    "4. Tiedon analysointi ja tutkiminen: Tutki dataa ja etsi siitä merkityksellisiä kaavoja, trendejä ja korrelaatioita. Visualisoi dataa tarvittaessa paremman ymmärryksen saamiseksi.\n",
    "\n",
    "5. Mallin valinta ja koulutus: Valitse sopiva koneoppimismalli ongelman ratkaisemiseksi ja kouluta se käyttäen koulutusdataa. Tämä voi sisältää päätöspuut, lineaarinen regressio, tukivektorikoneet, neuroverkot jne.\n",
    "\n",
    "6. Mallin arviointi: Arvioi mallin suorituskykyä testidataa käyttämällä. Käytä asiaankuuluvia suorituskyvyn mittareita, kuten tarkkuutta, tarkkuutta, tarkkuutta, tarkkuutta ja/tai virheellisten positiivisten ja negatiivisten suhteiden määriä.\n",
    "\n",
    "7. Mallin hienosäätö: Tarvittaessa hienosäädä mallia paremman suorituskyvyn saavuttamiseksi. Tämä voi sisältää hyperparametrien optimointia, ristiin validointia ja mallin uudelleenkoulutusta.\n",
    "\n",
    "8. Mallin käyttöönotto: Aseta koulutettu malli tuotantoympäristöön tai muuhun käyttötarkoitukseen. Seuraa mallin suorituskykyä ja suorita tarvittaessa jatkuvaa ylläpitoa ja päivityksiä.\n",
    "\n",
    "On olemassa useita viitekehyksiä ja malleja, jotka auttavat suunnittelemaan ja toteuttamaan datatiedeprojekteja tehokkaasti. Näitä ovat esimerkiksi CRISP-DM (Cross-Industry Standard Process for Data Mining) ja TDSP (Team Data Science Process). Nämä mallit tarjoavat rakenteen ja vaiheistuksen datatiedeprojektien suunnitteluun ja toteuttamiseen, mikä auttaa organisaatioita hallitsemaan projektejaan paremmin ja varmistamaan, että ne saavuttavat tavoitteensa tehokkaasti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko x: Yksinkertainen ennustaminen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arvioidaan Netflixin katsotuimpia ohjelmia päivittäin. Yritetään selvittää, vaikuttaako Netflixin yksinoikeus (exclusive) ohjelman suosioon, eli tässä tapauksessa sen päivien määrään top 10:ssä."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Lataa datasetti\n",
    "data = pd.read_csv('netflix_daily_top_10.csv')\n",
    "\n",
    "# Muuta Yes-arvo numeroksi 1 ja tyhjät arvot nollaksi\n",
    "data['Netflix Exclusive'] = data['Netflix Exclusive'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# Jaetaan data piirteisiin (X) ja kohdemuuttujaan (y)\n",
    "X = data[['Netflix Exclusive']]\n",
    "y = data['Days In Top 10']\n",
    "\n",
    "# Jaetaan data koulutus- ja testausjoukkoihin\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Koulutetaan lineaarinen regressiomalli\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Tehdään ennuste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Sitten visualisoidaan malli\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_test['Netflix Exclusive'], y=y_test, color='blue', label='Actual Data')\n",
    "sns.lineplot(x=X_test['Netflix Exclusive'], y=y_pred, color='red', label='Regression Line')\n",
    "plt.xlabel('Netflixin yksinoikeus (1=on, 0=ei)')\n",
    "plt.ylabel('Päivät Top 10:ssä')\n",
    "plt.title('Netflixin yksinoikeuden vaikutus päivien määrään Top 10:ssä')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visuaalisaation avulla voidaan (ehkä) arvioida, että ohjelmat ovat kauemmin top 10:ssä katsotuimmassa ohjelmassa, kun ne eivät ole Netflixin yksinoikeudella jakamia. Mallin tarkkuutta voidaan arvioida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Virhe on aika suuri, joten en ehkä luottaisi malliin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
