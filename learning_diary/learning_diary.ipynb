{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oppimispäiväkirja\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 1 - ohjelmien asennus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datatiedeprojektin keskeiset Python-kirjastot:\n",
    "- **Pandas**: Datan manipulaatio ja analyysi\n",
    "- **Scikit-learn**: Luokittelu- ja regressioalgoritmit\n",
    "- **Seaborn**: Datan visuaalinen analyysi\n",
    "- **Holoviews**: Datan visualisointi\n",
    "- **Streamlit**: Koneoppimisen ja datatieteen sovelluskehitys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miten kirjastot kannattaa asentaa?\n",
    "\n",
    "Kirjastojen asennukseen selkein tapa Windowsilla on käyttää command linea eli komentoriviä. Pip on Pythonin asennuksen mukana tuleva pakettien- ja kirjastojenhallintajärjestelmä, jonka avulla ohjelmat on helppo asentaa. Yleisesti kirjastojen asennus toimii komentorivikomennolla \"pip install -kirjaston nimi-\". Näin asensin Pandas-, Seaborn- ja Holoviews-kirjastot. Testataan, että asennukset onnistuivat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn\n",
    "import holoviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huomasin kuitenkin, että Scikit-learn ja Streamlit -kirjastot eivät tue 32-bittistä Windowsia ja Pythonia. En siis itse saanut niitä asennettua, mutta muilla käyttöjärjestelmillä asennus toimisi samalla tavalla kuin yllä. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erilaisia ympäristövaihtoehtoja ovat:\n",
    "- CSC Notebooks\n",
    "- Google Colab\n",
    "- Anaconda\n",
    "- Microsoft Visual Studio Code\n",
    "- Jupyter Lab\n",
    "- pilvipalvelupohjainen analytiikkaympäristö, esim. Google Cloud Platform tai Microsoft Azure\n",
    "\n",
    "**Mitkä ovat niiden vahvuudet ja heikkoudet?**\n",
    "\n",
    "Osa ympäristöistä tukevat pilvipalvelupohjaista virtuaalikoneella tapahtuvaa laskentaa ja tallennusta. Näitä ovat Google Colab ja pilvipalvelupohjaiset ympäristöt. Kun datan käsittely ja laskeminen tehdään muualla, säästyy omalta tietokoneelta resursseja ja myös reaaliaikainen pilveen tallennus on hyötynä. Jupyter Lab on Jupyter Notebookille tarkoitettu ohjelma, joka antaa käyttöön monia ominaisuuksia, kuten erilaisia visualisointeja ja laajan määrän kirjastoja. Jupyter Lab on saman organisaation kehittämä kuin Jupyter Notebook ja siksi se on varma vaihtoehto. Samoin Anaconda on suosittu järjestelmä, sillä se sisältää valmiiksi monia tarpeellisia kirjastoja, esimerkiksi pakettienhallintaan käytetyn conda-järjestelmän. CSC Notebooks on suomalaisen organisaation vaihtoehto, joka myös käyttää ulkoista laskentatietokonetta. Virtual Studio Code sisältää IDEn ja mahdollisuuden ladata monia kirjastoja helposti.\n",
    "\n",
    "Oikeastaan kaikki vaihtoehdot ovat hyviä ja on vaikea sanoa, mikä niistä olisi paras tai huonoin. Eniten eroja on siinä, käytetäänkö ulkoista laskentatehoa ja voiko sisältöä tallentaa helposti pilveen. Myös visuaalisissa puolissa on eroja.\n",
    "\n",
    "**Miten teet valinnan ympäristöjen välillä?**\n",
    "\n",
    "Valitsin itse käyttöön Visual Studio Coden, sillä olen käyttänyt sitä jo paljon aiemmin, eli käyttöönottoon ja opetteluun ei tarvitse käyttää aikaa turhaan. Pidän myös siitä, että mukana tulee kätevä ja selkeäkäyttöinen IDE ja VS Code tukee myös Gitin käyttöä samalla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miten dataa luetaan ja hallinnoidaan?\n",
    "\n",
    "Dataa voidaan lukea esimerkiksi taulukoista tai muista tiedostoista ja tietokannoista. Näitä tiedostoja voivat olla esim. CSV ja JSON -päätteiset tiedostot. Ennen kuin dataa voidaan alkaa analysoimaan, se täytyy esikäsitellä, eli siitä poistetaan epäkelvot ja kaksinkertaiset arvot, se normalisoidaan ja voidaan muotoilla. Sitten dataa voidaan analysoida eri keinoin, esimerkiksi tilastollisella analyysilla. Tätä analyysia voidaan myös visualisoida esimerkiksi yllä mainituilla kirjastoilla. Data-analyysissa tavoitteena voi olla mm. ennustaminen, luokittelu tai ongelmien ratkaiseminen. Dataa hallinoidaan ja säilytetään usein tietokannoissa tai taulukkomuodossa.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 2 - Aineiston kerääminen ja esikäsittely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tämän projektin datalähde\n",
    "\n",
    "Löysin mielenkiintoisen datalähteen Netflixin suosituimmista ohjelmista päivittäin. Dataa on saatavilla tammikuusta 2020 marraskuuhun 2022. Harmillisesti datalähdettä ei voi lukea linkistä suoraan nettisivulta, sillä sen lataaminen vaati sisäänkirjautumista. Datalähde on kuitenkin ladattuna samassa kansiossa kuin tämä päiväkirja nimellä 'netflix_daily_top_10.csv'. Datan voi myös löytää [tältä sivulta](https://www.kaggle.com/datasets/prasertk/netflix-daily-top-10-in-us?resource=download). \n",
    "\n",
    "Toinen datalähde, jota mahdollisesti käytän projektissa, on IMDB:n arvostelut ohjelmista. Datalähde löytyy tästä kansiosta nimellä 'title.ratings.tsv.gz'. Jotta arvostelujen lähde saadaan toimimaan, pitää lukea myös IMDB:n toista datalähdettä, josta saamme ohjelmien otsikot tekstimuodossa, sillä 'title.ratings.tsv.gz' -datalähteessä yksittäinen ohjelma on pelkästään identifioitu numerosarjalla. Tämä toinen IMDB:n datalähde on nimeltään 'title.basics.tsv.gz'. Molemmat IMDB:n datalähteet voi myös löytää [tältä sivulta](https://developer.imdb.com/non-commercial-datasets/).\n",
    "\n",
    "Ajatuksena olisi yhdistää näitä Netlixin ja IMDB:n datalähdeitä jotenkin. Tällä hetkellä ideana on verrata Netflixin ohjelmien päivittäistä suosiota IMDB:n arvostelunumeroihin ja löytää, onko näillä jokin suhde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datan lukeminen Pythonilla\n",
    "\n",
    "Pythonilla yleisin datan lukemiseen käytetty kirjasto on Pandas. Pandasilla voi lukea tietoa suoraan erilaisista datatiedostoista, esim. CSV ja JSON tai luoda oman dataluokan kaksiulotteisena taulukkona. Esimerkkinä luku Netflix-ohjelmien .csv-tiedostosta: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(\"netflix_daily_top_10.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luetaan data myös IMDB:n datalähteestä:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m netflix_titles_chunk \u001b[38;5;241m=\u001b[39m netflix_chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Read basics file and filter based on title column\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m basics_chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(basics_file, compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, usecols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtconst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimaryTitle\u001b[39m\u001b[38;5;124m'\u001b[39m], chunksize\u001b[38;5;241m=\u001b[39mbasics_chunksize):\n\u001b[0;32m     19\u001b[0m     basics_chunk_filtered \u001b[38;5;241m=\u001b[39m basics_chunk[basics_chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimaryTitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(netflix_titles_chunk)]\n\u001b[0;32m     20\u001b[0m     filtered_basics_chunks\u001b[38;5;241m.\u001b[39mappend(basics_chunk_filtered)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py:1624\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   1623\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1625\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1626\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py:1733\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1731\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[1;32m-> 1733\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     (\n\u001b[0;32m   1701\u001b[0m         index,\n\u001b[0;32m   1702\u001b[0m         columns,\n\u001b[0;32m   1703\u001b[0m         col_dict,\n\u001b[1;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\parsers.pyx:826\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\parsers.pyx:875\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "basics_file = 'title.basics.tsv.gz'\n",
    "ratings_file = 'title.ratings.tsv.gz'\n",
    "netflix_file = 'netflix_daily_top_10.csv'\n",
    "\n",
    "# Define a smaller chunk size for basics file\n",
    "basics_chunksize = 10 ** 1  # Adjust this number based on your available memory\n",
    "\n",
    "# Initialize an empty list to store the filtered basics chunks\n",
    "filtered_basics_chunks = []\n",
    "\n",
    "# Read Netflix file in chunks\n",
    "for netflix_chunk in pd.read_csv(netflix_file, usecols=['Title'], chunksize=basics_chunksize):\n",
    "    # Get unique titles from the current Netflix chunk\n",
    "    netflix_titles_chunk = netflix_chunk['Title'].unique()\n",
    "\n",
    "    # Read basics file and filter based on title column\n",
    "    for basics_chunk in pd.read_csv(basics_file, compression='gzip', sep='\\t', usecols=['tconst', 'primaryTitle'], chunksize=basics_chunksize):\n",
    "        basics_chunk_filtered = basics_chunk[basics_chunk['primaryTitle'].isin(netflix_titles_chunk)]\n",
    "        filtered_basics_chunks.append(basics_chunk_filtered)\n",
    "\n",
    "# Concatenate the filtered basics chunks into a single DataFrame\n",
    "filtered_basics_df = pd.concat(filtered_basics_chunks)\n",
    "\n",
    "# Read ratings file, no need to filter based on title column since we're merging with filtered_basics_df later\n",
    "ratings_df = pd.read_csv(ratings_file, compression='gzip', sep='\\t')\n",
    "\n",
    "# Merge filtered_basics_df with ratings_df on the 'tconst' column\n",
    "merged_df = pd.merge(ratings_df, filtered_basics_df, on='tconst', how='inner')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muita vaihtoehtoja ovat esimerkiksi Pythonin moduuli \"csv\", jonka reader-funktion avulla voidaan lukea CSV-tiedostoja seuraavanlaisesti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.reader(\"tiedoston nimi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythonissa on myös toinen sisäänrakennettu paketti \"json\", jonka avulla JSON-tiedostoja voi muuttaa Pythonin dictionaryn muotoon. Muutos toimii seuraavasti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(\"tiedoston nimi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Molemmat Pythonin omat moduulit / paketit ovat kuitenkin vaikeampia käyttää kuin Pandas, sillä niissä data pitää käydä rivi kerrallaan läpi, eikä esimerkiksi kokoa taulukon tulostukseen ole suoraa funktiota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datan esittäminen ja DataFrame\n",
    "\n",
    "Koneoppimisprojekteissa data esitetään usein taulukkomuodossa, jossa jokainen rivi vastaa yhtä havaintoa tai esimerkkiä, ja jokainen sarakkeista vastaa havainnon piirrettä tai ominaisuutta. Tämä taulukkomuotoinen esitys voi olla DataFrame, joka on Pandas-kirjaston tarjoama tietorakenne Pythonissa.  Valitussa Netflix-datasetissa jokainen rivi on yksi havainto eli päivä ja jokainen sarake päivän ohjelmien ominaisuus, (esimerkiksi ohjelman nimi).\n",
    "\n",
    "DataFrame on taulukko, joka koostuu riveistä ja sarakkeista. Se tarjoaa kätevän tavan järjestää, analysoida ja manipuloida dataa Pythonissa. DataFrame on erityisen hyödyllinen koneoppimisprojekteissa, koska se mahdollistaa datan lataamisen, esikäsittelyn, visualisoinnin ja mallin sovittamisen helposti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datan hallinta ja to_pickle()\n",
    "\n",
    "To_pickle() -metodi tarjoaa kätevän tavan tallentaa DataFrame-objekti binäärimuodossa, mikä helpottaa tiedon hallintaa. Tallennus on kätevää, kun DataFramen haluaa tallentaa väliaikaisesti tai jakaa sen muille. Pkl-muotoinen tiedosto on myös nopeampi lukea ja tallentaa kuin esimerkiksi csv-tiedosto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
